{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlling RNNs with constant input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from Synthesis import *\n",
    "from visualizations import *\n",
    "from torch import Tensor\n",
    "from typing import Tuple, Any\n",
    "import os\n",
    "\n",
    "# Pytorch settings\n",
    "default_dtype = torch.float64\n",
    "torch.set_default_dtype(default_dtype)\n",
    "dvc = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Figure settings\n",
    "renaming = {\"backward_nominal_state\": \"Backward nominal state\", \"forward_nominal_state\": \"Forward nominal state\",\n",
    "            \"backward_push\": \"Backward push\", \"forward_pull\": \"Forward pull\", \"linearized\": \"Linearized at x0\", \"linearized_origin\": \"Linearized at origin\",\n",
    "            \"backward_initial_state\": \"Backward initial state\", \"forward_final_state\": \"Forward final state\", \"naive\": \"Naive\"}\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "plt.rcParams.update({'figure.figsize': (8, 6)})\n",
    "plt.rcParams.update({'figure.dpi': 300})\n",
    "figdir = \"figures\"\n",
    "os.makedirs(figdir, exist_ok=True)\n",
    "\n",
    "# Pandas output settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Data output directory\n",
    "datdir = \"data\"\n",
    "os.makedirs(datdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment-related functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get `x1` given the model, horizon, and initial state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x1(mdl: nn.Module, x0: Tensor, B: Tensor, T: float = 1.,\n",
    "           scaling: float = 1, method: str = \"random\", odeint_kwargs = {}) -> Tensor:\n",
    "    \"\"\"Get target state `x1`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mdl : nn.Module\n",
    "        The model to synthesize control for.\n",
    "    x0 : Tensor (nTrials, d)\n",
    "        The initial state of the system.\n",
    "    B : Tensor (d, k)\n",
    "        The input matrix for the control synthesis.\n",
    "    T : float\n",
    "        The time horizon for the control synthesis.\n",
    "    scaling : float\n",
    "        Scaling factor. Default is 1.\n",
    "    method : str\n",
    "        The method to use for generating the target state. Options are:\n",
    "        - \"random\": Normal distribution with mean 0 and std `scaling`.\n",
    "        - \"perturbation\": Add a random perturbation to `x0` with std\n",
    "            `scaling`.\n",
    "        - \"drift_perturbation\": Add a random perturbation with std\n",
    "            `scaling` to the drift final state `PhiTx0`.\n",
    "        - \"numeric_reachable\": Use a random input with std `scaling`\n",
    "            to get the controlled final state at time `T`.\n",
    "        - \"approx_reachable\": Use the approximation of reachable set\n",
    "            given by the forward nominal state synthesis where the\n",
    "            random vector has zero mean and std `scaling`. Detailed\n",
    "            in later sections. Only valid for input matrix consisting\n",
    "            one-hot vectors.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x1 : Tensor (nTrials, d)\n",
    "        The target state.\n",
    "    \"\"\"\n",
    "    if method == \"random\":\n",
    "        return torch.randn_like(x0) * scaling  # Random target state\n",
    "    elif method == \"perturbation\":\n",
    "        return x0 + torch.randn_like(x0) * scaling\n",
    "    \n",
    "    dvc = x0.device\n",
    "    N = x0.shape[-1]\n",
    "    old_shape = x0.shape\n",
    "    x0 = x0.reshape(-1, N)\n",
    "    fullT = torch.tensor([0, T], dtype=default_dtype).to(dvc)\n",
    "\n",
    "    if method == \"drift_perturbation\":\n",
    "        PhiTx0 = odeint(mdl, x0, fullT, **odeint_kwargs)[-1]\n",
    "        x1 = PhiTx0 + torch.randn_like(PhiTx0) * scaling\n",
    "        x1 = x1.reshape(old_shape)\n",
    "        return x1\n",
    "    \n",
    "    elif method == \"numeric_reachable\":\n",
    "        u = torch.randn(x0.shape[0], B.shape[-1], device=dvc, dtype=default_dtype) * scaling  # Random input\n",
    "        cMdl = ControlledMdl(mdl, u @ B.T)  # Controlled model with random input\n",
    "        x1 = odeint(cMdl, x0, fullT, **odeint_kwargs)[-1]  # Simulate to get target state\n",
    "        x1 = x1.reshape(old_shape)\n",
    "        return x1\n",
    "    \n",
    "    elif method == \"approx_reachable\":\n",
    "        DN = vmap(jacrev(lambda x: mdl(None, x)))\n",
    "        idx = torch.linalg.vector_norm(B, dim=-1) < 1e-8\n",
    "\n",
    "        PhiTx0 = odeint(mdl, x0, fullT, **odeint_kwargs)[-1]\n",
    "\n",
    "        if torch.any(idx):        \n",
    "            DNPhiTx0 = DN(PhiTx0)  # (nTrials, d, d)\n",
    "            ATx0 = torch.linalg.solve(torch.linalg.matrix_exp(T * DNPhiTx0) - torch.eye(N, dtype=default_dtype, device=dvc), DNPhiTx0)  # (nTrials, d, d)\n",
    "            M = ATx0[..., idx, :]  # (nTrials, d - k, d)\n",
    "            Q, _ = torch.linalg.qr(torch.permute(M, [0, 2, 1]), mode=\"complete\")  # (nTrials, d, d), (nTrials, d, d - k)\n",
    "            Q2 = Q[..., -(M.shape[-1] - M.shape[-2]):, ]  # (nTrials, d, k)\n",
    "            xi = torch.randn([Q2.shape[0], Q2.shape[-1], 1], device=dvc, dtype=default_dtype) * scaling  # (nTrials, k, 1)\n",
    "            x1 = PhiTx0 + torch.squeeze(Q2 @ xi, -1)  # (nTrials, d)\n",
    "            x1 = x1.reshape(old_shape)\n",
    "        else:\n",
    "            x1 = PhiTx0 + torch.randn_like(PhiTx0) * scaling\n",
    "\n",
    "        return x1\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method {method} for generating target state x1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to obtain synthesized input and endpoint error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_err(mdl: nn.Module, x0: Tensor, x1: Tensor, T: float = 30, B: Tensor | None = None,\n",
    "            methods: Sequence[str] = (\"backward_nominal_state\", \"forward_nominal_state\", \"linearized\"),\n",
    "            odeint_kwargs: dict = {}, **kwargs) -> Tuple[dict, dict, dict]:\n",
    "    \"\"\"\n",
    "    Get the performance measures for the control synthesis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mdl : nn.Module\n",
    "        The model to synthesize control for.\n",
    "    x0 : Tensor\n",
    "        The initial state of the system.\n",
    "    x1 : Tensor\n",
    "        The target state of the system.\n",
    "    T : float\n",
    "        The time horizon for the control synthesis.\n",
    "    B : Tensor | None\n",
    "        The input matrix for the control synthesis. If None, assumed to be an identity matrix.\n",
    "    methods : Sequence[str]\n",
    "        The methods to use for control synthesis. Default is (\"backward_push\", \"forward_pull\", \"linearized\").\n",
    "    odeint_kwargs : dict\n",
    "        Additional keyword arguments to pass to the ODE solver.\n",
    "    kwargs : dict\n",
    "        Additional keyword arguments to pass to the synthesis function.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    err : dict (with `methods` as keys)\n",
    "        Relative endpoint error for each method. Norm(xt - x1) / Norm(x1 - x0).\n",
    "    Mcond : dict (with `methods` as keys)\n",
    "        Condition number of the matrix being inverted for each method.\n",
    "    ivp_error : dict (with `methods` as keys)\n",
    "        Relative error for the initial value problem for each method.\n",
    "        - \"backward_push\": Norm(PhiTPsiTx1 - x1) / Norm(x1)\n",
    "        - \"forward_pull\": Norm(PsiTPhiTx0 - x0) / Norm(x0)\n",
    "        - others: None\n",
    "    \"\"\"\n",
    "    err = {}\n",
    "    Mcond = {}\n",
    "    ivp_error = {}\n",
    "    for method in methods:\n",
    "        err[method] = []\n",
    "        I, Mcond[method], ivp_err = synthesize(mdl, x0, x1, T,\n",
    "            method=method, odeint_kwargs=odeint_kwargs, **kwargs)\n",
    "        if B is not None:\n",
    "            res = torch.linalg.lstsq(B, I.T, rcond=None)  # B: d x k, I: Nsamples x d\n",
    "            u = res.solution  # u: k x Nsamples\n",
    "            I = u.T @ B.T  # I: Nsamples x d\n",
    "        c_mdl = ControlledMdl(mdl, I)\n",
    "        xt = odeint(c_mdl, x0,\n",
    "            torch.tensor([0, T], dtype=default_dtype, device=x0.device), **odeint_kwargs)[-1]\n",
    "        err[method] = torch.norm(xt - x1, dim=-1) / torch.norm(x1 - x0, dim=-1)\n",
    "        if method == \"backward_push\":\n",
    "            ivp_error[method] = ivp_err / torch.norm(x1, dim=-1)\n",
    "        elif method == \"forward_pull\":\n",
    "            ivp_error[method] = ivp_err / torch.norm(x0, dim=-1)\n",
    "        else:\n",
    "            ivp_error[method] = [None for _ in range(x0.shape[0])]\n",
    "        if Mcond[method] is None:\n",
    "            Mcond[method] = [None for _ in range(x0.shape[0])]\n",
    "    return err, Mcond, ivp_error\n",
    "\n",
    "\n",
    "def get_error_summary(mdlfun: Callable | Sequence, allT: Sequence[float] = [2. ** e for e in range(-2, 8)], N: int = 100,\n",
    "                      nMdlPerT: int = 5, nSimPerMdl: int = 20, dvc: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                      Bfun: Callable | Sequence | None = None, x1fun: Callable | None | str = \"random\", scaling: float = 1.,\n",
    "                      methods: Sequence[str] = (\"backward_push\", \"forward_pull\", \"linearized\"),\n",
    "                      odeint_kwargs: dict = {}, shuffle_mdls: bool = True, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the relative endpoint error for a sequence of models and times\n",
    "\n",
    "    `mdlfun` should be either a function which accepts a parameter `N` and returns a model,\n",
    "    or a sequence of models. `Bfun` should be either None (identity input matrix) or the\n",
    "    same type as `mdlfun`.\n",
    "\n",
    "    If `mdlfun` is a sequence, `shuffle_mdls` determines whether to randomly select models.\n",
    "    If `shuffle_mdls` is False, the first `nMdlPerT` models will be used for each time `T`.\n",
    "\n",
    "    If `x1fun` is a function, it should take the model `mdl`, the initial state `x0`,\n",
    "    input matrix `B`, and time `T`. If it is a string, it should be one of the option of\n",
    "    `get_x1` method.\n",
    "    \"\"\"\n",
    "    \n",
    "    Bs = [None for _ in range(nMdlPerT)]\n",
    "    if callable(mdlfun):\n",
    "        fs = [mdlfun(N) for i in range(nMdlPerT)]\n",
    "        if Bfun is not None:\n",
    "            Bs = [Bfun(N) for i in range(nMdlPerT)]\n",
    "    else:\n",
    "        if shuffle_mdls:\n",
    "            rp = torch.randperm(len(mdlfun), dtype=int)[:nMdlPerT]\n",
    "        else:\n",
    "            rp = range(nMdlPerT)\n",
    "        fs = [mdlfun[i] for i in rp]\n",
    "        if Bfun is not None:\n",
    "            Bs = [Bfun[i] for i in rp]\n",
    "    \n",
    "    failed = {}  # To handle ODE integration underflow errors\n",
    "    \n",
    "    dfs = [pd.DataFrame(columns=[\"T\", \"method\", \"model_idx\", \"experiment_idx\", \"error\", \"Mcond\", \"ivp_error\"])]\n",
    "    for T in allT:\n",
    "        print(f\"T={T}\")\n",
    "        for i in range(nMdlPerT):\n",
    "            mdl = fs[i].to(dvc)\n",
    "            B = Bs[i]\n",
    "            if B is not None:\n",
    "                B = B.to(dvc)\n",
    "            x0 = torch.randn(nSimPerMdl, N, device=dvc, dtype=default_dtype)\n",
    "            if isinstance(x1fun, Callable):\n",
    "                x1 = x1fun(mdl, x0, B, T).to(dvc)\n",
    "            else:\n",
    "                x1 = get_x1(mdl, x0, B, T, method=x1fun, scaling=scaling).to(dvc)\n",
    "            for method in methods:\n",
    "                if method in failed and failed[method]:\n",
    "                    continue\n",
    "                try:\n",
    "                    err, Mcond, ivp_error = get_err(mdl, x0, x1, T, B, methods=[method], odeint_kwargs=odeint_kwargs, **kwargs)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error for T={T}, method={method}: {e}\")\n",
    "                    failed[method] = True\n",
    "                    continue\n",
    "                for erri, mcondi, ivpi, idx in zip(err[method], Mcond[method], ivp_error[method], range(nSimPerMdl)):\n",
    "                    dfs.append(pd.DataFrame({\n",
    "                        \"T\": T, \"method\": method, \"error\": erri.item(),\n",
    "                        \"Mcond\": mcondi.item() if mcondi is not None else None,\n",
    "                        \"ivp_error\": ivpi.item() if ivpi is not None else None,\n",
    "                        \"model_idx\": i + 1, \"experiment_idx\": idx + 1,\n",
    "                        \"scaling\": scaling, \"x1_method\": x1fun if isinstance(x1fun, str) else \"custom\"\n",
    "                        }, index=[0]))\n",
    "    \n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to simulate the model without input, with inputs synthesized using nonlinear synthesis, or inputs using linearized synthesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj(mdl: nn.Module, x0: Tensor, x1: Tensor, T: float = 30., **kwargs) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Get synthesis and simulate model\n",
    "    \n",
    "    Inputs are the same as `synthesize` function but without `method`.\n",
    "\n",
    "    Ouputs:\n",
    "    - `I`: Tensor of shape `(3, nTrials, N)`. The first slice is all zeros, the second\n",
    "        and the third contains the control synthesis using nonlinear and linearized\n",
    "        methods, respectively.\n",
    "    - `traj`: Tensor of shape `(3, nSteps, nTrials, N)`. Simulated trajectories under the\n",
    "        input given by `I`.\n",
    "    \"\"\"\n",
    "    nSteps = 100  # Just for visualization\n",
    "\n",
    "    dvc = x0.device\n",
    "\n",
    "    # Synthesize control\n",
    "    I_drift = torch.zeros(x0.shape, device=dvc)\n",
    "    I_nl = synthesize(mdl, x0, x1, T, method=\"backward_push\", **kwargs)[0]\n",
    "    I_lin = synthesize(mdl, x0, x1, T, method=\"linearized\", **kwargs)[0]\n",
    "    mdl_c_nl = ControlledMdl(mdl, I_nl)\n",
    "    mdl_c_lin = ControlledMdl(mdl, I_lin)\n",
    "\n",
    "    # Simulate trajectories\n",
    "    t = torch.linspace(0, T, nSteps, device=dvc)\n",
    "    x_drift = odeint(mdl, x0, t, **kwargs)\n",
    "    x_nl = odeint(mdl_c_nl, x0, t, **kwargs)\n",
    "    x_lin = odeint(mdl_c_lin, x0, t, **kwargs)\n",
    "\n",
    "    return torch.stack([I_drift, I_nl, I_lin]), torch.stack([x_drift, x_nl, x_lin])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to visualize and compare trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_traj(traj, x1, axes=None, **kwargs):\n",
    "    \"\"\"Visualize trajectories obtained by `get_traj` function\"\"\"\n",
    "    if axes is None:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(12, 5))\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    traj = traj.cpu().detach().numpy()\n",
    "    x1 = x1.cpu().detach().numpy()\n",
    "    x0 = traj[0, 0, :, :]  # (nTrials, N)\n",
    "    \n",
    "    err = np.squeeze(traj[:, -1, :, :] - x1)  # (3, nTrials, N)\n",
    "    err = np.linalg.norm(err, axis=-1, keepdims=False) / np.linalg.norm(x1 - x0, axis=-1, keepdims=False)  # (3, nTrials)\n",
    "    err_med = np.median(err, axis=-1, keepdims=False)\n",
    "    err_max = np.max(err, axis=-1, keepdims=False)\n",
    "    \n",
    "    for i in range(3):\n",
    "        PlotTraj(traj[i], x1, PCspace=traj[0], ax=axes[i], legend=False, **kwargs)\n",
    "    axes[0].set_title(f\"drift\")\n",
    "    axes[1].set_title(f\"nonlinear synthesis, relative error:\\nmedian: {err_med[1]:.3g}, max: {err_max[1]:.3g}\")\n",
    "    axes[2].set_title(f\"linearized synthesis, relative error:\\nmedian: {err_med[2]:.3g}, max: {err_max[2]:.3g}\")\n",
    "\n",
    "    return axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to visualize and compare endpoint errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_error(df: pd.DataFrame, ax=None, **kwargs) -> plt.Axes:\n",
    "    \"\"\"Visualize the error dataframe\"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    allT = sorted(df[\"T\"].unique())\n",
    "    df[\"log_error\"] = np.log10(df[\"error\"])\n",
    "    df[\"method\"] = df[\"method\"].map(renaming)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 4.5))\n",
    "    else:\n",
    "        fig = ax.get_figure()\n",
    "    hue_key = \"model\" if \"model\" in df.columns and len(df[\"model\"].unique()) > 1 else \"method\"\n",
    "    style_key = \"method\" if hue_key == \"model\" else None\n",
    "    ax = sns.lineplot(data=df, x=\"T\", y=\"log_error\", hue=hue_key, style=style_key, ax=ax, **kwargs)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xticks(allT)\n",
    "    ax.set_xticklabels(allT)\n",
    "    ax.set_xlabel(\"Time horizon\")\n",
    "    ax.set_ylabel(\"Log10 relative error\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_error_violin(df: pd.DataFrame, ax=None, **kwargs) -> plt.Axes:\n",
    "    \"\"\"Visualize the error dataframe using violin plots\"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    allT = sorted(df[\"T\"].unique())\n",
    "    df[\"log_error\"] = np.log10(df[\"error\"])\n",
    "    df[\"method\"] = df[\"method\"].map(renaming)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 4.5))\n",
    "    else:\n",
    "        fig = ax.get_figure()\n",
    "    \n",
    "    if \"hue_key\" not in kwargs:\n",
    "        hue_key = \"method\"\n",
    "    else:\n",
    "        hue_key = kwargs.pop(\"hue_key\")\n",
    "    do_split = (\"model\" in df and len(df[\"model\"].unique()) == 2)\n",
    "    ax = sns.violinplot(data=df, x=\"T\", y=\"log_error\", hue=hue_key, split=do_split, ax=ax, inner=\"quart\", **kwargs)\n",
    "    ax.set_xlabel(\"Time horizon\")\n",
    "    ax.set_ylabel(\"Log10 relative error\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical experiments for B = Id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the overall simulation configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allT = [2. ** e for e in range(-2, 7)]  # 0.25 to 64\n",
    "all_scaling = [0.05, 0.1, 0.5, 1]\n",
    "x1_method = \"drift_perturbation\"\n",
    "nMdlPerT, nSimPerMdl, N = 5, 20, 100\n",
    "methods = [\"backward_nominal_state\", \"forward_nominal_state\", \"backward_initial_state\", \"forward_final_state\",\n",
    "           \"backward_push\", \"forward_pull\", \"linearized\", \"linearized_origin\", \"naive\"]\n",
    "odeint_kwargs = {\"atol\": 1e-14, \"rtol\": 1e-13, \"method\": \"dopri8\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct stable or unstable linear models by setting the maximum of the real parts of the eigenvalues to be -0.1 or 0.1, respectively. Below is the eigenspectrum of one example stable and one example unstable model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_mdls = [get_random_linear(100, max_eig=-0.1, dvc=dvc), get_random_linear(100, max_eig=0.1, dvc=dvc)]\n",
    "tmp_names = [\"Stable linear\", \"Unstable linear\"]\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 6), sharex=True, sharey=True)\n",
    "for (i, ax) in enumerate(axes):\n",
    "    eigvals = torch.linalg.eigvals(tmp_mdls[i].W - torch.diag(tmp_mdls[i].D)).cpu().detach().numpy()\n",
    "    ax.scatter(eigvals.real, eigvals.imag)\n",
    "    ax.set_title(tmp_names[i])\n",
    "for ax in axes:\n",
    "    ax.hlines(0, ax.get_xlim()[0], ax.get_xlim()[1], color=\"black\", linestyle=\"--\")\n",
    "    ax.vlines(0, ax.get_ylim()[0], ax.get_ylim()[1], color=\"black\", linestyle=\"--\")\n",
    "\n",
    "fig.supxlabel(\"Real part\")\n",
    "fig.supylabel(\"Imaginary part\")\n",
    "plt.suptitle(\"Eigenvalues of random linear models\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(figdir, \"linear_example_eigvals.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stable_linear = lambda N, **kwargs: get_random_linear(N, max_eig=-0.1, **kwargs)\n",
    "get_unstable_linear = lambda N, **kwargs: get_random_linear(N, max_eig=0.1, **kwargs)\n",
    "\n",
    "dfs = []\n",
    "for sc in all_scaling:\n",
    "    print(f\"Getting error summary for stable linear models with scaling {sc}...\")\n",
    "    df_stable = get_error_summary(get_stable_linear, methods=methods,\n",
    "                                  nMdlPerT=nMdlPerT, nSimPerMdl=nSimPerMdl, allT=allT, N=N, dvc=dvc,\n",
    "                                  odeint_kwargs=odeint_kwargs, x1fun=x1_method, scaling=sc)\n",
    "    df_stable['model'] = 'Stable linear'\n",
    "    print(f\"Getting error summary for unstable linear models with scaling {sc}...\")\n",
    "    df_unstable = get_error_summary(get_unstable_linear, methods=methods,\n",
    "                                    nMdlPerT=nMdlPerT, nSimPerMdl=nSimPerMdl, allT=allT, N=N, dvc=dvc,\n",
    "                                    odeint_kwargs=odeint_kwargs, x1fun=x1_method, scaling=sc)\n",
    "    df_unstable['model'] = 'Unstable linear'\n",
    "    dfs.append(df_stable)\n",
    "    dfs.append(df_unstable)\n",
    "\n",
    "df_linear = pd.concat(dfs, ignore_index=True)\n",
    "df_linear.to_csv(os.path.join(datdir, \"Linear_error.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization on log-log scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = vis_error(\n",
    "    df_linear[\n",
    "        (df_linear[\"method\"].isin([\"linearized\", \"linearized_origin\", \"naive\"])) &\n",
    "        (df_linear[\"scaling\"] == 1)\n",
    "    ]\n",
    ")\n",
    "ax.set_title(f\"Error of control synthesis for random linear models\\n{nMdlPerT} models per time horizon, {nSimPerMdl} simulations per model\")\n",
    "plt.savefig(os.path.join(figdir, \"Linear_error.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: errorbars indicate 95% confidence intervals computed over the 100 trials across 5 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MINDy models\n",
    "\n",
    "#### Show some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mindys = get_all_MINDys(dvc=dvc)\n",
    "\n",
    "nTrials = 10  # Number of trials\n",
    "T = 80.  # Time horizon\n",
    "\n",
    "nMdls = 4  # Number of models to visualize\n",
    "mdl_idx = np.random.choice(len(mindys), nMdls, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(nMdls, 3, figsize=(12, 18), subplot_kw={\"projection\": \"3d\"})\n",
    "for idx, axv in zip(mdl_idx, axes):\n",
    "    x0, x1 = torch.randn(nTrials, N, device=dvc), torch.randn(nTrials, N, device=dvc)\n",
    "    _, traj = get_traj(mindys[idx], x0, x1, T)\n",
    "    vis_traj(traj, x1, axes=axv)\n",
    "    title = axv[0].get_title()\n",
    "    axv[0].set_title(f\"Model {idx}, {title}\")\n",
    "\n",
    "fig.legend(axes[-1, -1].get_lines()[:4], ['Start', 'End', 'Trajectory', 'Target'])\n",
    "fig.suptitle(f\"Control synthesis for MINDy models with nonlinear or linearized methods\\nTime horizon = {T:.1f} units\")\n",
    "plt.savefig(os.path.join(figdir, \"MINDy_traj.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly selected four models and compare the endpoint error using three different inputs: zero input (drift), synthesis for nonlinear system, and the synthesis for the \"linearized\" system (if `x0` is not a fixed point, we counteract the drift using the input)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarizing plots for MINDy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mindys = get_all_MINDys(dvc=dvc)\n",
    "\n",
    "dfs = []\n",
    "for sc in all_scaling:\n",
    "    print(f\"Getting error summary for MINDy models with scaling {sc}...\")\n",
    "    df_MINDy = get_error_summary(mindys, methods=methods, nMdlPerT=nMdlPerT,\n",
    "                                 nSimPerMdl=nSimPerMdl, allT=allT, N=N, dvc=dvc,\n",
    "                                 odeint_kwargs=odeint_kwargs, x1fun=x1_method, scaling=sc)\n",
    "    df_MINDy['model'] = 'MINDy'\n",
    "    dfs.append(df_MINDy)\n",
    "\n",
    "df_MINDy = pd.concat(dfs, ignore_index=True)\n",
    "df_MINDy.to_csv(os.path.join(datdir, \"MINDy_error.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the results for all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = vis_error(df_MINDy[df_MINDy[\"scaling\"] == 0.05])\n",
    "ax.set_title(f\"Error of control synthesis for MINDy models\\n{nMdlPerT} models per time horizon, {nSimPerMdl} simulations per model\")\n",
    "plt.savefig(os.path.join(figdir, \"MINDy_error.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random tanh RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate random RNNs with tanh activation functions according to [(Schuessler et al., 2020)](https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.2.013111). The eigenspectrum of the Jacobian of the model at the origin will contain a bulk centered at (-1, 0) with a radius of `g` and several outliers, where `g` is a scaling factor of the random part of connectivity and the outliers are determined by the low-rank component.\n",
    "\n",
    "We have three types of RNNs:\n",
    "\n",
    "1. An RNN with `g = 0.9` (\"edge of chaos\") and low-rank component rank `K = 1`. No correlation between the low-rank component and the random part. This is commonly used in the literature. The dynamics are usually monostable.\n",
    "2. Similar to the first one, but the low-rank component is correlated with the random part, leading to an eigenvalue outlier that usually makes the dynamics bistable.\n",
    "3. An RNN with `g = 0.5` and no low-rank component. We found that this type of RNNs has a spectral norm slightly smaller than 1, thus satisfying the spectral norm condition $|\\!|\\!|W|\\!|\\!| < \\lambda_{\\min}(D)$. This can improve the performance of forward synthesis. Particularly, the initial value problem error is reduced.\n",
    "\n",
    "Example eigenspectrum and drift dynamics were shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to generate monostable/bistable/smallnorm models\n",
    "get_monostable = lambda N, **kwargs: get_RNN(N, K=1, g=0.9, theta_i=[], **kwargs)\n",
    "get_bistable = lambda N, **kwargs: get_RNN(N, K=1, g=0.9, theta_i=[1.2], **kwargs)\n",
    "get_smallnorm = lambda N, **kwargs: get_RNN(N, K=0, g=0.5, theta_i=[], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpN = 100\n",
    "mdl1 = get_monostable(tmpN, dvc=dvc)\n",
    "mdl2 = get_bistable(tmpN, dvc=dvc)\n",
    "mdl3 = get_smallnorm(tmpN, dvc=dvc)\n",
    "titles = [\"Monostable\", \"Bistable\", \"Small norm\"]\n",
    "x0 = torch.randn(10, tmpN, device=dvc)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "for (i, mdl) in enumerate([mdl1, mdl2, mdl3]):\n",
    "\n",
    "    sims = odeint(mdl, x0, torch.linspace(0, 50, 100, device=dvc))\n",
    "    J = jacobian(lambda x: mdl(0., x), torch.zeros(tmpN, device=dvc))\n",
    "    eigvals = torch.linalg.eigvals(J).cpu().detach().numpy()\n",
    "    ax = fig.add_subplot(2, 3, i + 1)\n",
    "    ax.scatter(eigvals.real, eigvals.imag)\n",
    "    ax.set_title(titles[i] + \"\\nEigenspectrum of Jacobian at x=0\", fontsize=9)\n",
    "    ax.hlines(0, ax.get_xlim()[0], ax.get_xlim()[1], color=\"black\", linestyle=\"--\")\n",
    "    ax.vlines(0, ax.get_ylim()[0], ax.get_ylim()[1], color=\"black\", linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Real part\", fontsize=7)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Imaginary part\", fontsize=7)\n",
    "\n",
    "    ax = fig.add_subplot(2, 3, i + 4, projection=\"3d\")\n",
    "    PlotTraj(sims, ax=ax)\n",
    "    ax.set_title(\"Example trajectories\", fontsize=9)\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.suptitle(\"Random tanh RNN model\")\n",
    "plt.savefig(os.path.join(figdir, \"Tanh_example.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for sc in all_scaling:\n",
    "    print(f\"Getting error summary for monostable tanh RNN with scaling {sc}...\")\n",
    "    df_mono = get_error_summary(get_monostable, methods=methods, nMdlPerT=nMdlPerT,\n",
    "                                nSimPerMdl=nSimPerMdl, allT=allT, N=N, dvc=dvc,\n",
    "                                odeint_kwargs=odeint_kwargs, x1fun=x1_method, scaling=sc)\n",
    "    df_mono['model'] = 'Monostable tanh RNN'\n",
    "    print(f\"Getting error summary for bistable tanh RNN with scaling {sc}...\")\n",
    "    df_bi = get_error_summary(get_bistable, methods=methods, nMdlPerT=nMdlPerT,\n",
    "                              nSimPerMdl=nSimPerMdl, allT=allT, N=N, dvc=dvc,\n",
    "                              odeint_kwargs=odeint_kwargs, x1fun=x1_method, scaling=sc)\n",
    "    df_bi['model'] = 'Bistable tanh RNN'\n",
    "    print(f\"Getting error summary for small norm tanh RNN with scaling {sc}...\")\n",
    "    df_smallnorm = get_error_summary(get_smallnorm, methods=methods, nMdlPerT=nMdlPerT,\n",
    "                                     nSimPerMdl=nSimPerMdl, allT=allT, N=N, dvc=dvc,\n",
    "                                     odeint_kwargs=odeint_kwargs, x1fun=x1_method, scaling=sc)\n",
    "    df_smallnorm['model'] = 'Small norm tanh RNN'\n",
    "    dfs.append(df_mono)\n",
    "    dfs.append(df_bi)\n",
    "    dfs.append(df_smallnorm)\n",
    "\n",
    "df_tanh = pd.concat(dfs, ignore_index=True)\n",
    "df_tanh.to_csv(os.path.join(datdir, \"Tanh_error.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = vis_error(df_tanh[\n",
    "    (df_tanh[\"method\"].isin([\"forward_nominal_state\", \"linearized\", \"naive\"])) &\n",
    "    (df_tanh[\"scaling\"] == 0.05)\n",
    "])\n",
    "ax.set_title(f\"Error of control synthesis for tanh RNN models\\n{nMdlPerT} models per time horizon, {nSimPerMdl} simulations per model\")\n",
    "plt.savefig(os.path.join(figdir, \"Tanh_error.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exceptional cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting that for RNNs with more complicated dynamics (e.g., if the low rank component satisfies $n^{\\top}Jm > 0$), sometimes the endpoint error is larger. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = get_RNN(100, K=1, theta_i=[1.2, 1], dvc=dvc)\n",
    "x0 = torch.randn(30, 100, device=dvc)\n",
    "x1 = torch.randn(30, 100, device=dvc)\n",
    "T = 30.\n",
    "I = synthesize(mdl, x0, x1, T)[0]\n",
    "\n",
    "sims = odeint(mdl, x0, torch.linspace(0, T, 100, device=dvc))\n",
    "\n",
    "c_mdl = ControlledMdl(mdl, I)\n",
    "c_sims = odeint(c_mdl, x0, torch.linspace(0, T, 100, device=dvc))\n",
    "xt = c_sims[-1]\n",
    "err = torch.norm(xt - x1, dim=-1) / torch.norm(x1, dim=-1)\n",
    "err = torch.log10(err).cpu().detach().numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(1, 3, 1, projection=\"3d\")\n",
    "PlotTraj(sims, ax=ax)\n",
    "ax.set_title(\"Drift\")\n",
    "ax = fig.add_subplot(1, 3, 2, projection=\"3d\")\n",
    "PlotTraj(c_sims, x1, sims, ax=ax)\n",
    "ax.set_title(\"Controlled\")\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "sns.histplot(err, ax=ax)\n",
    "ax.set_title(\"Log10 relative error\")\n",
    "\n",
    "plt.suptitle(\"Control synthesis for more complex tanh RNN model\")\n",
    "plt.savefig(os.path.join(figdir, \"Tanh_exceptional.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of endpoint error results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "fs = [\"Linear_error.csv\", \"MINDy_error.csv\", \"Tanh_error.csv\"]\n",
    "for f in fs:\n",
    "    df = pd.read_csv(os.path.join(datdir, f))\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(df)\n",
    "df.to_csv(os.path.join(datdir, \"error_summary.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with underactuated systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use input matrix `B` that has rank smaller than the state space. We will not use MINDy models here, as the dimensions in MINDy models are not interchangeable. Instead, we plan to use 128-dimensional random tanh RNNs that either satisfy the spectral norm condition (`smallnorm`) or not (`bistable`). We will consider time horizons up to 16 for simplicity, and we will consider input dimension $k$ from 1 to 128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Approximately) reachable cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we choose random $x^0$, but select $x^1$ such that the synthesized input $I$ is in the range of $B$. In principle, such $x^1$ should be approximately reachable by $u = B^{+}I$. We will only use the forward nominal state synthesis here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we construct $x^1$ given $x^0$, $B = [e_{i_{1}}, e_{i_{2}}, \\dots, e_{i_{k}}]$ (where $e_{i}$ is the $i$-th canonical vector in $\\mathbb{R}^{d}$) and $T$ is the following:\n",
    "\n",
    "1. Compute the matrix $\\mathcal A_T(x^0)=[e^{TDN(\\Phi_T(x^0))}-\\text{Id}]^{-1}DN(\\Phi_T(x^0))$.\n",
    "2. Let $M\\in\\mathbb{R}^{(d - k)\\times d}$ be the matrix consisting of the rows of $\\mathcal{A}_{T}(x^{0})$ where the corresponding rows of $B$ are zeros.\n",
    "3. Compute the full QR decomposition of $M^{\\top}$, i.e., $M^{\\top} = Q R$ with $Q\\in\\mathbb{R}^{d\\times d}$ and $R\\in\\mathbb{R}^{d\\times (d - k)}$, with the last $k$ rows of $R$ being zero.\n",
    "4. The null space of $M$ is spanned by the last $k$ columns of $Q$, denoted as $Q_{2}$\n",
    "5. Then $v$ is given by $v = Q_{2}\\xi$ where $\\xi$ is a random vector in $\\mathbb{R}^{k}$.\n",
    "6. Set $x^{1} = \\Phi_{T}(x^{0}) + v$ and run the synthesis.\n",
    "\n",
    "This is implemented in the `'approx_reachable'` option of the `get_x1` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allT = [2. ** e for e in range(-2, 5)]\n",
    "allK = [1, 16, 32, 64, 96, 120, 126, 128]\n",
    "all_scaling = [0.1, 1]\n",
    "x1_method = \"approx_reachable\"  # Use the approximation of reachable set\n",
    "nMdlPerT, nSimPerMdl, N = 10, 30, 128\n",
    "methods = [\"forward_nominal_state\", \"linearized\"]\n",
    "odeint_kwargs = {\"atol\": 1e-14, \"rtol\": 1e-13, \"method\": \"dopri8\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the models and input matrices ahead of time as we need to compare within the same model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdls_smallnorm = [get_smallnorm(N, dvc=dvc) for _ in range(nMdlPerT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for k in allK:\n",
    "    Bs = [torch.eye(N, device=dvc, dtype=default_dtype)[:, :k] for _ in range(nMdlPerT)]\n",
    "    for sc in all_scaling:\n",
    "        print(f\"K={k}, scaling={sc}...\")\n",
    "        print(f\"Smallnorm tanh RNN with One-hot input matrix...\")\n",
    "        df = get_error_summary(mdls_smallnorm, Bfun=Bs, methods=methods, x1fun=x1_method, scaling=sc,\n",
    "            nMdlPerT=nMdlPerT, nSimPerMdl=nSimPerMdl, shuffle_mdls=False,\n",
    "            allT=allT, N=N, dvc=dvc, odeint_kwargs=odeint_kwargs)\n",
    "        df['model'] = \"Small norm tanh RNN\"\n",
    "        df['K'] = k\n",
    "        df['input_type'] = \"One-hot\"\n",
    "        dfs.append(df)\n",
    "\n",
    "df_NonIdInput = pd.concat(dfs, ignore_index=True)\n",
    "df_NonIdInput.to_csv(os.path.join(datdir, \"NonIdInput_approx_reachable_error.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerically reachable cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we choose $x^{1}$ by selecting some random $u$ and numerically integrating the model to get $x^{1}$.\n",
    "\n",
    "This is implemented in the `'numeric_reachable'` option of the `get_x1` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allT = [2. ** e for e in range(-2, 4)]\n",
    "allK = [1, 16, 32, 64, 96, 120, 126, 128]\n",
    "all_scaling = [0.1, 1]\n",
    "x1_method = \"numeric_reachable\"  # Use the numerically obtained reachable set\n",
    "nMdlPerT, nSimPerMdl, N = 10, 30, 128\n",
    "methods = [\"forward_nominal_state\", \"linearized\"]\n",
    "odeint_kwargs = {\"atol\": 1e-14, \"rtol\": 1e-13, \"method\": \"dopri8\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the models and input matrices ahead of time as we need to compare within the same model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdls_smallnorm = [get_smallnorm(N, dvc=dvc) for _ in range(nMdlPerT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for k in allK:\n",
    "    Bs = [torch.eye(N, device=dvc, dtype=default_dtype)[:, :k] for _ in range(nMdlPerT)]\n",
    "    for sc in all_scaling:\n",
    "        print(f\"K={k}, scaling={sc}...\")\n",
    "        print(f\"Smallnorm tanh RNN with One-hot input matrix...\")\n",
    "        df = get_error_summary(mdls_smallnorm, Bfun=Bs, methods=methods, x1fun=x1_method, scaling=sc,\n",
    "            nMdlPerT=nMdlPerT, nSimPerMdl=nSimPerMdl, shuffle_mdls=False,\n",
    "            allT=allT, N=N, dvc=dvc, odeint_kwargs=odeint_kwargs)\n",
    "        df['model'] = \"Small norm tanh RNN\"\n",
    "        df['K'] = k\n",
    "        df['input_type'] = \"One-hot\"\n",
    "        df['scaling'] = sc\n",
    "        dfs.append(df)\n",
    "\n",
    "df_NonIdInput = pd.concat(dfs, ignore_index=True)\n",
    "df_NonIdInput.to_csv(os.path.join(datdir, \"NonIdInput_numeric_reachable_error.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not necessarily reachable cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we choose random $x^0$ and $x^1$ for the synthesis. Because $B$ is not full rank, there is no guarantee that the two-point boundary value problem is solvable. We want to use this experiment to show how big the reachable set is, and how accurate can the synthesis be when the endpoint is not necessarily reachable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allT = [2. ** e for e in range(-2, 5)]\n",
    "allK = [1, 16, 32, 64, 96, 120, 126, 128]\n",
    "x1_method = \"random\"  # Use random target state\n",
    "nMdlPerT, nSimPerMdl, N = 10, 100, 128\n",
    "methods = [\"backward_nominal_state\", \"forward_nominal_state\", \"linearized\", \"naive\"]\n",
    "odeint_kwargs = {\"atol\": 1e-14, \"rtol\": 1e-13, \"method\": \"dopri8\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the models and input matrices ahead of time as we need to compare within the same model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdls_bistable = [get_bistable(N, dvc=dvc) for _ in range(nMdlPerT)]\n",
    "mdls_smallnorm = [get_smallnorm(N, dvc=dvc) for _ in range(nMdlPerT)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for k in allK:\n",
    "    print(f\"K={k}...\")\n",
    "    Bs1 = [torch.eye(N, device=dvc, dtype=default_dtype)[:, :k] for _ in range(nMdlPerT)]\n",
    "    Bs2 = [torch.randn(N, k, device=dvc, dtype=default_dtype) for _ in range(nMdlPerT)]\n",
    "    for (Bs, input_type) in zip([Bs1, Bs2], ['One-hot', 'Random']):\n",
    "        for (mdl_fun, mdl_name) in zip([mdls_bistable, mdls_smallnorm], [\"Bistable tanh RNN\", \"Small norm tanh RNN\"]):\n",
    "            print(f\"{mdl_name} with {input_type} input matrix...\")\n",
    "            df = get_error_summary(mdl_fun, Bfun=Bs, methods=methods, x1fun=x1_method, scaling=1.,\n",
    "                nMdlPerT=nMdlPerT, nSimPerMdl=nSimPerMdl, shuffle_mdls=False,\n",
    "                allT=allT, N=N, dvc=dvc, odeint_kwargs=odeint_kwargs)\n",
    "            df['model'] = mdl_name\n",
    "            df['K'] = k\n",
    "            df['input_type'] = input_type\n",
    "            dfs.append(df)\n",
    "\n",
    "df_NonIdInput = pd.concat(dfs, ignore_index=True)\n",
    "df_NonIdInput.to_csv(os.path.join(datdir, \"NonIdInput_error.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We benchmarked the run time for linear and tanh RNNs respectively, and analyze how it changes with the number of neurons `N`, the time horizon `T`, and the number of samples `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependence on the number of neurons `N`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allN = [32, 64, 128, 256, 512]\n",
    "T = 10.\n",
    "n = 1\n",
    "n_rep = 5\n",
    "\n",
    "times = [pd.DataFrame(columns=[\"N\", \"time\", \"Model\"])]\n",
    "for N in allN:\n",
    "    for _ in range(n_rep):\n",
    "        print(f\"N = {N}, Rep {_}\")\n",
    "        mdl = get_RNN(N, dvc=dvc)\n",
    "        x0 = torch.randn(n, N, device=dvc)\n",
    "        x1 = torch.randn(n, N, device=dvc)\n",
    "        time = timeit(lambda: synthesize(mdl, x0, x1, T, method=\"forward_nominal_state\"), number=1)\n",
    "        times.append(pd.DataFrame({\"N\": N, \"time\": time, \"Model\": \"RNN\"}, index=[0]))\n",
    "\n",
    "        mdl = get_random_linear(N, dvc=dvc)\n",
    "        time = timeit(lambda: synthesize(mdl, x0, x1, T, method=\"linearized\"), number=1)\n",
    "        times.append(pd.DataFrame({\"N\": N, \"time\": time, \"Model\": \"Linear\"}, index=[0]))\n",
    "\n",
    "df = pd.concat(times, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(datdir, \"synthesis_time_N.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_time'] = np.log10(df['time'])\n",
    "sns.lineplot(data=df, x=\"N\", y=\"log_time\", hue=\"Model\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"log10 Time (s)\")\n",
    "plt.title(f\"Time for control synthesis, {n} trial(s), time horizon = {T}\")\n",
    "plt.savefig(os.path.join(figdir, \"time_comparison_dim.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependence on the number of trials `n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alln = [1, 4, 16, 64, 256, 1024]\n",
    "T = 10.\n",
    "N = 100\n",
    "n_rep = 5\n",
    "\n",
    "times = [pd.DataFrame(columns=[\"n\", \"time\", \"Model\"])]\n",
    "for n in alln:\n",
    "    for _ in range(n_rep):\n",
    "        print(f\"n = {n}, Rep {_}\")\n",
    "        mdl = get_RNN(N, dvc=dvc)\n",
    "        x0 = torch.randn(n, N, device=dvc)\n",
    "        x1 = torch.randn(n, N, device=dvc)\n",
    "        time = timeit(lambda: synthesize(mdl, x0, x1, T, method=\"forward_nominal_state\"), number=1)\n",
    "        times.append(pd.DataFrame({\"n\": n, \"time\": time, \"Model\": \"RNN\"}, index=[0]))\n",
    "\n",
    "        mdl = get_random_linear(N, dvc=dvc)\n",
    "        time = timeit(lambda: synthesize(mdl, x0, x1, T, method=\"linearized\"), number=1)\n",
    "        times.append(pd.DataFrame({\"n\": n, \"time\": time, \"Model\": \"Linear\"}, index=[0]))\n",
    "\n",
    "df = pd.concat(times, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(datdir, \"synthesis_time_nTrials.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_time'] = np.log10(df['time'])\n",
    "sns.lineplot(data=df, x=\"n\", y=\"log_time\", hue=\"Model\")\n",
    "plt.xlabel(\"Number of trials\")\n",
    "plt.ylabel(\"log10 Time (s)\")\n",
    "plt.title(f\"Time for control synthesis, {N} features, time horizon = {T}\")\n",
    "plt.savefig(os.path.join(figdir, \"time_comparison_nTrials.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, sometimes nonlinear synthesis can be faster than linear synthesis (note that they are for different systems). This might be due to the fact that under some error tolerance, neural ODE can be solved with fewer iterations than the computation needed to compute matrix exponential for linear synthesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependence on the time horizon `T`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allT = [2. ** i for i in range(-2, 7)]\n",
    "N = 100\n",
    "n = 20\n",
    "n_rep = 5\n",
    "\n",
    "times = [pd.DataFrame(columns=[\"T\", \"time\", \"Model\"])]\n",
    "\n",
    "for T in allT:\n",
    "    for _ in range(n_rep):\n",
    "        print(f\"T = {T}, Rep {_}\")\n",
    "        mdl = get_RNN(N, dvc=dvc)\n",
    "        x0 = torch.randn(n, N, device=dvc)\n",
    "        x1 = torch.randn(n, N, device=dvc)\n",
    "        try:\n",
    "            time = timeit(lambda: synthesize(mdl, x0, x1, T, method=\"forward_nominal_state\"), number=1)\n",
    "            times.append(pd.DataFrame({\"T\": T, \"time\": time, \"Model\": \"RNN\"}, index=[0]))\n",
    "        except Exception as e:\n",
    "            print(f\"Error for T={T}, RNN: {e}\")\n",
    "\n",
    "        mdl = get_random_linear(N, dvc=dvc)\n",
    "        try:\n",
    "            time = timeit(lambda: synthesize(mdl, x0, x1, T, method=\"linearized\"), number=1)\n",
    "            times.append(pd.DataFrame({\"T\": T, \"time\": time, \"Model\": \"Linear\"}, index=[0]))\n",
    "        except Exception as e:\n",
    "            print(f\"Error for T={T}, Linear: {e}\")\n",
    "\n",
    "df = pd.concat(times, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(datdir, \"synthesis_time_T.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_time'] = np.log10(df['time'])\n",
    "ax = sns.lineplot(data=df, x=\"T\", y=\"log_time\", hue=\"Model\")\n",
    "plt.xscale(\"log\")\n",
    "ax.set_xticks(allT)\n",
    "ax.set_xticklabels(allT)\n",
    "plt.xlabel(\"Time horizon\")\n",
    "plt.ylabel(\"log10 Time (s)\")\n",
    "plt.title(f\"Time for control synthesis, {n} trial(s), {N} features\")\n",
    "plt.savefig(os.path.join(figdir, \"time_comparison_T.pdf\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
